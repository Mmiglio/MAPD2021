{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5efb5e9d",
   "metadata": {},
   "source": [
    "# Lecture 2: Spark Dataframes\n",
    "\n",
    "Spark SQL is a module that can be used to process structured data, e.g organized in columns. Spark Dataframes relies on RDD, but provides richer optimizations under the hoods.\n",
    "\n",
    "\n",
    "As for the previous lecture, select the most appropriate variable based on where this notebook is run. If the docker cluster is used, the process of starting spark cluster can be skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1411e39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set this variable with one of the following values\n",
    "# -> 'local'\n",
    "# -> 'docker_container'\n",
    "# -> 'docker_cluster'\n",
    "CLUSTER_TYPE ='docker_container'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ae4928f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CLUSTER_TYPE=docker_container\n"
     ]
    }
   ],
   "source": [
    "# set env variable\n",
    "%env CLUSTER_TYPE $CLUSTER_TYPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bff73e",
   "metadata": {},
   "source": [
    "## Start the cluster \n",
    "\n",
    "Environment variables need to be set only in the case of a local cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6937e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLUSTER_TYPE=='local':\n",
    "    import findspark\n",
    "    findspark.init('/Users/matteo/Work/MAPD/spark-3.1.1-bin-hadoop3.2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefa682c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash --no-raise-error\n",
    "\n",
    "if [[ \"$CLUSTER_TYPE\" != \"docker_cluster\" ]]; then\n",
    "    echo \"Launching master and worker\"\n",
    "    \n",
    "    # start master \n",
    "    $SPARK_HOME/sbin/start-master.sh --host localhost \\\n",
    "        --port 7077 --webui-port 8080\n",
    "    \n",
    "    # start worker\n",
    "    $SPARK_HOME/sbin/start-worker.sh spark://localhost:7077 \\\n",
    "        --cores 2 --memory 2g\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9515fef",
   "metadata": {},
   "source": [
    "## Create the Spark session\n",
    "\n",
    "Later on we will explain what is the role of [Apache Arrow](), but first we need to install it and create the spark session with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0965c117",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2011070",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "if CLUSTER_TYPE in ['local', 'docker_container']:\n",
    "    \n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"spark://localhost:7077\")\\\n",
    "        .appName(\"First spark application\")\\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"false\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "elif CLUSTER_TYPE == 'docker_cluster':\n",
    "    \n",
    "    # use the provided master\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"spark://spark-master:7077\")\\\n",
    "        .appName(\"First spark application\")\\\n",
    "        .config(\"spark.executor.memory\", \"512m\")\\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"false\")\\\n",
    "        .getOrCreate()\n",
    "    \n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a3bdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc9fdcb",
   "metadata": {},
   "source": [
    "### Load data\n",
    "\n",
    "By default Spark can create a dataframe from data stored in many formats such as `csv`, `json` and many other listed [here](https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html). \n",
    "\n",
    "If your dataset is stored in a format that spark cannot understand, it is always possible to create a dataframe from an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b08f6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import Row\n",
    "import numpy as np\n",
    "\n",
    "def read_custom_data(file_name):\n",
    "    # pretend to load some real data\n",
    "    \n",
    "# read some files in parallel and create a dataframe\n",
    "file_list = ['file1', 'file2']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bb184e",
   "metadata": {},
   "source": [
    "## Dimuon\n",
    "\n",
    "Several particles decay in a pair of opposite charged leptons (electrons, muons and taus).\n",
    "The dimuon spectrum, computed by calculating the invariant mass of muon pairs with opposite charge, features the presence of a number of narrow resonances, corresponding to the mass of the parent particle: from the η meson at about 548 MeV  up to the Z boson at about 91 GeV.\n",
    "Rare processes are also associated to this very same final state, such as the Bs dimuon decay (first observed in 2012 at CMS and LHCb), and the elusive Higgs dimuon decay (for which there is statistical evidence, but not yet an observation); new yet undiscovered particles might also show up as new resonances in the dimuon spectrum as the statistics and accelerator energy is increased.\n",
    "\n",
    "![Event Display](imgs/lecture2/event_display.png)\n",
    "\n",
    "The dataset used in this exercise is taken from the CERN Open Data portal (https://opendata.cern.ch/) and represents a portion of the data collected by the CMS collaboration at the Large Hadron Collider in 2010.\n",
    "This dataset comprise of only the fraction of events retained by online selections (trigger) which identify collisions where muons have been produced, thus storing only about 10 events out of the about 40 millions of collisions produced every second at LHC.\n",
    "The whole dataset collected by the CMS collaboration since the start of LHC operations in 2010 is comprised of tens of PBs of data and simulations.\n",
    "\n",
    "\n",
    "A subset of events have been extracted and preprocessed since original data are stored in ROOT files containing. The result is stored as JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea32e9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset on dataset/lecture2/dimuon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43b161f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce00c6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show first rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048ce826",
   "metadata": {},
   "source": [
    "As we will see in the next cells, the API used to interact with spark dataframes is very similar to Pandas\n",
    "\n",
    "**Note**: [Koalas](https://github.com/databricks/koalas) aims at replicating 1 to 1 the pandas API in Apache Spark. This package will be included by default in the next version of Spark (Spark 3.2). However, what we will se today are the core concepts of pyspark's dataframes and will remain mandatory to work with it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36a9f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of events per each sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262dbcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# persist the dataframe to avoid\n",
    "# loading it every time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dd2d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of muons\n",
    "# distribution and plot it\n",
    "num_muons_dist = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327ee0bd",
   "metadata": {},
   "source": [
    "Plotting this data would require a lot of code, but fortunately there is a magic trick we can use: it is possible to convert a spark dataframe into a  Pandas Dataframe with `toPandas()`. Again, as it happens for `collect()`, the entire dataset is fetched into the master and it may not fit in the memory.\n",
    "\n",
    "**Apache Arrow** comes into play in this contex: it is an in-memory columnar data format that is used in spark to efficiently transfer data between the JVM and Python processes. When it is not enabled Spark comunicates with python processes by serializing and deserializing one element of the time. With Arrow this operation is \"vectorized\", i.e. one column at the time is transfered. This is possible because both spark and pandas included Arrow representation.\n",
    "\n",
    "[Here](https://xuechendi.github.io/2019/04/16/Apache-Arrow) you can find a nice blog post explaining how it works.\n",
    "\n",
    "In this way, operations between Pyspark and Pandas are more efficient and we can try to get the best from both worlds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4542b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# punt the results into a pandas dataframe\n",
    "num_muons_dist = \n",
    "\n",
    "num_muons_dist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbf7180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type of this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa88eb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "mc_counts = num_muons_dist.loc[num_muons_dist['sample']=='mc']\n",
    "plt.bar(\n",
    "    mc_counts['nMuon'], mc_counts['count'],\n",
    "    label = 'Montecarlo', width=1, alpha=0.6\n",
    ")\n",
    "\n",
    "data_counts = num_muons_dist.loc[num_muons_dist['sample']=='data']\n",
    "plt.errorbar(\n",
    "    data_counts['nMuon'],data_counts['count'],\n",
    "    yerr = np.sqrt(data_counts['count']),\n",
    "    label = 'Data', color='black', fmt='o'\n",
    ")\n",
    "plt.xlim(-0.5,16.5)\n",
    "plt.xlabel(\"Number of muons\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.legend()\n",
    "plt.semilogy()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6afbb8d",
   "metadata": {},
   "source": [
    "We are interested in dimuon events, i.e. events having 2 muons. We can do this with `filter` or `where`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bfeb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only events with two muons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df289bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# possible to perform the same selection in multiple ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12188fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset is not flat\n",
    "dimuon_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6164685a",
   "metadata": {},
   "source": [
    "Now we can make our dataset flat for further processing: in this way we will be able to perform operations between the columns.\n",
    "\n",
    "A new column can be created by using `withColumn(name, func)` where `func` is a function taking as input one or more columns. An arbitrary complex function can be used to produce the new column: for example we can define a User Defined Function (UDF) which will be applied to every row of the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a02b623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create columns for the two muons\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53000ba",
   "metadata": {},
   "source": [
    "We can now continue with our work and select only the events where the two muons have opposite charge (column `c1` and `c2`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e883119d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select candidates with opposit charge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0aee3ab",
   "metadata": {},
   "source": [
    "To compute the invariant mass of the two candidate muons we need first to compute the four-momentum of the parent particle \n",
    "\n",
    "$$\n",
    "p = \\left(E_1+E_2,\\quad p_{x,1}+p_{x,2},\\quad p_{y,1}+p_{y,2},\\quad p_{z,1}+p_{z,2}\\right) = (E,\\, \\mathbf{p})\n",
    "$$\n",
    "\n",
    "Create four new columns in the dataframe, one per each coordinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf6f28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new columns for the four momentum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0221439",
   "metadata": {},
   "source": [
    "From the 4-momentum of the candidate we can compute the invariant mass as\n",
    "\n",
    "$$\n",
    "M = \\sqrt{p\\cdot p} =  \\sqrt{(E^2 - \\|\\mathbf{p}\\|^2)} = \\sqrt{(E^2 - (px^2 + py^2 + pz^2)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b78f542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute invariant mass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d16d17",
   "metadata": {},
   "source": [
    "It is possible to obtain the same result using a UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e423ee27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "import math\n",
    "\n",
    "# obtain the same result using udf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a16c40d",
   "metadata": {},
   "source": [
    "Similarly, we can use pandas UDF. This allows to process pyspark's dataframes as pandas dataframes and pandas series.\n",
    "They result particularly efficient in the groupped maps, i.e. by appling a function after a groupby. More on this can be found [here](https://spark.apache.org/docs/latest/api/python/user_guide/arrow_pandas.html#grouped-map). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92ec311",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ff7edf",
   "metadata": {},
   "source": [
    "Computer the mean energy for different groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bdb8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute mean energy for each sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac71c956",
   "metadata": {},
   "source": [
    "### Discovering particles\n",
    "\n",
    "Now that all the interesting quantities have been computed we can use them to perform the analysis.\n",
    "\n",
    "For example, let's plot the invariant mass of the dimuon system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b3dbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the histogram of M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb17dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute it for each sample\n",
    "histogram_results = {}\n",
    "for sample in ['mc', 'data']:\n",
    "    histogram_results[sample] = {}\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27af3bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "mc_res = histogram_results['mc']\n",
    "bin_centers = mc_res['bins'][:-1] + np.diff(mc_res['bins'])/2\n",
    "plt.hist(\n",
    "    bin_centers, weights=mc_res['counts'], bins=mc_res['bins'],\n",
    "    label='Montecarlo', alpha=0.6\n",
    ")\n",
    "\n",
    "data_res = histogram_results['data']\n",
    "bin_centers = data_res['bins'][:-1] + np.diff(data_res['bins'])/2\n",
    "\n",
    "plt.errorbar(\n",
    "    bin_centers, data_res['counts'], yerr=np.sqrt(data_res['counts']),\n",
    "    fmt='o', ms=4, lw=1, color='black', label='data'\n",
    ")\n",
    "\n",
    "plt.xlabel(\"$m_{\\mu \\mu}$ [GeV]\")\n",
    "plt.ylabel(\"Events/0.5GeV\")\n",
    "plt.legend()\n",
    "plt.semilogy()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22638e5b",
   "metadata": {},
   "source": [
    "In the range 80-100 GeV it appears there is a large resonance, we can histogram the data in that range to see it better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8531ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_results = {}\n",
    "for sample in ['mc', 'data']:\n",
    "    histogram_results[sample] = {}\n",
    "    \n",
    "    bins, counts = (\n",
    "        dimuon_flat.where(col('sample')==sample)\n",
    "        .select('M')\n",
    "        .rdd.map(lambda x: x['M'])\n",
    "        .histogram(list(np.arange(70,115,0.5)))\n",
    "    )\n",
    "    \n",
    "    histogram_results[sample]['bins'] = bins\n",
    "    histogram_results[sample]['counts'] = counts\n",
    "    \n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "mc_res = histogram_results['mc']\n",
    "bin_centers = mc_res['bins'][:-1] + np.diff(mc_res['bins'])/2\n",
    "plt.hist(\n",
    "    bin_centers, weights=mc_res['counts'], bins=mc_res['bins'],\n",
    "    label='Montecarlo', alpha=0.6\n",
    ")\n",
    "\n",
    "data_res = histogram_results['data']\n",
    "bin_centers = data_res['bins'][:-1] + np.diff(data_res['bins'])/2\n",
    "\n",
    "plt.errorbar(\n",
    "    bin_centers, data_res['counts'], yerr=np.sqrt(data_res['counts']),\n",
    "    fmt='o', ms=4, lw=1, color='black', label='data'\n",
    ")\n",
    "\n",
    "plt.xlabel(\"$m_{\\mu \\mu}$ [GeV]\")\n",
    "plt.ylabel(\"Events/0.5GeV\")\n",
    "plt.legend()\n",
    "#plt.semilogy()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6725980e",
   "metadata": {},
   "source": [
    "We can try to perform some selection, to improve the quality of the signal and remove background. To do this we can try look at candidates with a large transverse momentum \n",
    "\n",
    "$$\n",
    "p_T = \\sqrt{p_x^2 + p_y^2}\n",
    "$$\n",
    "\n",
    "create a colum with name \"energetic\" and for each row set the value to `True` if $p_T\\geq 30$ Gev. Plot then the results and see if there is any improvement. You can compare just the data or montecarlo. \n",
    "\n",
    "This can be achieved in spark using `when()` and `otherwise()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e3ba42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8d6e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_histogram(dataset, column):\n",
    "    histogram_results = {}\n",
    "    for sample in ['mc', 'data']:\n",
    "        histogram_results[sample] = {}\n",
    "\n",
    "        bins, counts = (\n",
    "            dataset.where(col('sample')==sample)\n",
    "            .select(column)\n",
    "            .rdd.map(lambda x: x[column])\n",
    "            .histogram(list(np.arange(70,115,0.5)))\n",
    "        )\n",
    "\n",
    "        histogram_results[sample]['bins'] = bins\n",
    "        histogram_results[sample]['counts'] = counts\n",
    "        \n",
    "    return histogram_results\n",
    "        \n",
    "\n",
    "is_energetic = compute_histogram(dimuon_flat.where(col('highpt')==True), 'M')\n",
    "not_energetic = compute_histogram(dimuon_flat.where(col('highpt')==False), 'M')\n",
    " \n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "mc_res = not_energetic['mc']\n",
    "bin_centers = mc_res['bins'][:-1] + np.diff(mc_res['bins'])/2\n",
    "plt.hist(\n",
    "    bin_centers, weights=mc_res['counts'], bins=mc_res['bins'],\n",
    "    label='Not energetic', alpha=0.6\n",
    ")\n",
    "\n",
    "mc_res = is_energetic['mc']\n",
    "bin_centers = mc_res['bins'][:-1] + np.diff(mc_res['bins'])/2\n",
    "plt.hist(\n",
    "    bin_centers, weights=mc_res['counts'], bins=mc_res['bins'],\n",
    "    label='Energetic', alpha=0.6\n",
    ")\n",
    "\n",
    "plt.xlabel(\"$m_{\\mu \\mu}$ [GeV]\")\n",
    "plt.ylabel(\"Events/0.5GeV\")\n",
    "plt.legend()\n",
    "#plt.semilogy()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c341d6",
   "metadata": {},
   "source": [
    "## Stop worker and master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f99e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ab3d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash --no-raise-error\n",
    "\n",
    "if [[ \"$CLUSTER_TYPE\" != \"docker_cluster\" ]]; then\n",
    "    # stop worker \n",
    "    $SPARK_HOME/sbin/stop-worker.sh\n",
    "    \n",
    "    # start master\n",
    "    $SPARK_HOME/sbin/stop-master.sh\n",
    "fi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
